---
title: Learning Notes on NLP
date: 2018-01-27 23:32:09
tags: [Natural Language Processing]
categories: [Natural Language Processing]
---

### 1.熵
随机事件的不确定性叫做“熵”. 信息量的量化度量叫做“熵”. 变量的不确定性越大，熵就越大，要把它搞清楚所需要的信息量也就越大.
```
H(X) = -∑P(x)logP(x)
```
条件熵公式如下:
```
H(X|Y) = -∑P(x,y)logP(x|y)
```

### 2.互信息
两个随机事件的相关性的量化度量叫做“互信息”.  
```
I(X;Y) = H(X) - H(X|Y)
```
互信息是熵与条件熵的差, 取值在0到min(H(X), H(Y))之间，当X和Y完全相关时，它的取值为H(X)（查看条件熵公式即可理解），且H(X) = H(Y)；当两者完全无关时，它的取值为0。

### 3.相对熵（也叫 交叉熵）
相对熵用于衡量两个取值为正数的函数的相关性.  
+ 对于两个完全相同的函数，它们的相对熵为0
+ 相对熵越大，两个函数的差异越大；反之，相对熵越小，差异越小
+ 对于概率分布或者概率密度函数，如果取值均大于0，相对熵可以度量两个随机分布的差异性

### 4. 马尔可夫假设
马尔可夫为了简化问题，提出了一种简化的假设：即随机过程中各个状态`S_t`的概率分布，只与它的前一个状态`S_t-1`有关，即`P(S_t|S_1, S_2,..., S_t-1) = P(S_t|S_t-1)`。  
这个假设后来被命名为马尔可夫假设，而符合这个假设的随机过程称为马尔可夫过程，也称为马尔可夫链。  
[数学之美 P53]  
