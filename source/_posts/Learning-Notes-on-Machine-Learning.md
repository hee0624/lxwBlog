---
title: Learning Notes on Machine Learning
date: 2018-06-08 10:30:47
tags: [Machine Learning]
categories: [Machine Learning]
---
## 1.batch_size如何选择？
**线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆**(抛物面从上往下压缩就成了等高线)。对于多层神经元、非线性网络，在局部依然近似是抛物面。  

**在合理范围内，增大 batch_size 有何好处?**
+ 内存利用率提高了，大矩阵乘法的并行化效率提高
+ 跑完一次 epoch(全数据集)所需的迭代次数减少，对于相同数据量的处理速度进一步加快
+ 在一定范围内，一般来说 batch_size 越大，其确定的下降方向越准，**引起训练震荡越小**

**盲目增大 batch_size 有何坏处？**  
+ 内存利用率提高了，但是内存容量可能撑不住了
+ 跑完一次 epoch(全数据集)的迭代次数减少，**要想达到相同的精度，其所花费的时间大大增加了(所需要的 epoch 数量越来越多)，从而对参数的修正也就显得更加缓慢**
+ batch_size 增大到一定程度，其确定的下降方向已经基本不再变化

**总结:**
+ batch_size 太小，算法收敛速度太慢
+ 随着 batch_size 增大，处理相同数据量的速度越快
+ 随着 batch_size 增大，达到相同精度所需要的 epoch 数量越来越多
+ 由于上述两种因素的矛盾， batch_size 增大到某个时候，达到时间上的最优
+ 由于最终收敛精度会陷入不同的局部极值，因此 batch_size 增大到某些时候，达到最终收敛精度上的最优

batch_size会影响模型性能，过大或者过小都不合适。**设置过大的batch_size，可能会对训练时网络的准确性产生负面影响，因为它降低了梯度下降的随机性。**  
_**怎么做?**_  
要在可接受的训练时间内，确定最小的batch_size。一个能合理利用GPU并行性能的batch_size可能不会达到最佳的准确率，因为在有些时候，**较大的batch_size可能需要训练更多迭代周期才能达到相同的正确率**。  
**在开始时，要大胆地尝试很小的batch_size，如16、8，甚至是1**。  
_**为什么?**_  
**较小的batch_size能带来有更多起伏、更随机的权重更新**。这有两个积极的作用，一是能帮助训练"跳出"之前可能卡住它的局部最小值，二是能让训练在"平坦"的最小值结束，这通常会带来更好的泛化性能。

**References:**  
[深度机器学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260) 

## 2. 几个问题和技巧汇总
1). 关于验证集的loss曲线和acc曲线震荡，不平滑问题
出现loss震荡不平滑的原因可能如下:
+ 学习率可能太大
+ batch size太小
+ 样本分布不均匀

2). 如果训练中发现loss的值为NAN，这时可能的原因如下:
+ 学习率太高
+ 如果是自定义的损失函数，这时可能是所设计的损失函数有问题

3). 一般来说，较高的acc对应的loss较低，但这不是绝对，毕竟他们是两个不同的东西, 有时候即使acc高，但是loss值也高.

**References:**  
[keras中的一些小tips（一）](https://blog.csdn.net/momaojia/article/details/72877953) 

