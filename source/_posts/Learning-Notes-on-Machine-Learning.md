---
title: Learning Notes on Machine Learning
date: 2018-06-08 10:30:47
tags: [Machine Learning]
categories: [Machine Learning]
---
## 1.batch_size如何选择？
**线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆**(抛物面从上往下压缩就成了等高线)。对于多层神经元、非线性网络，在局部依然近似是抛物面。  

**在合理范围内，增大 batch_Size 有何好处?**
+ 内存利用率提高了，大矩阵乘法的并行化效率提高
+ 跑完一次 epoch(全数据集)所需的迭代次数减少，对于相同数据量的处理速度进一步加快
+ 在一定范围内，一般来说 batch_Size 越大，其确定的下降方向越准，引起训练震荡越小

**盲目增大 batch_Size 有何坏处？**  
+ 内存利用率提高了，但是内存容量可能撑不住了
+ 跑完一次 epoch(全数据集)所需的迭代次数减少，**要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢**
+ batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化

**总结:**
+ batch_Size 太小，算法收敛速度太慢
+ 随着 batch_Size 增大，处理相同数据量的速度越快
+ 随着 batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多
+ 由于上述两种因素的矛盾， batch_Size 增大到某个时候，达到时间上的最优
+ 由于最终收敛精度会陷入不同的局部极值，因此 batch_Size 增大到某些时候，达到最终收敛精度上的最优

batch_Size会影响模型性能，过大或者过小都不合适。设置过大的batch_Size，可能会对训练时网络的准确性产生负面影响，因为它降低了梯度下降的随机性。  
**怎么做?**  
要在可接受的训练时间内，确定最小的batch_Size。一个能合理利用GPU并行性能的批次大小可能不会达到最佳的准确率，因为在有些时候，较大的batch_Size可能需要训练更多迭代周期才能达到相同的正确率。  
在开始时，要大胆地尝试很小的batch_Size，如16、8，甚至是1。  
**为什么?**  
较小的batch_Size能带来有更多起伏、更随机的权重更新。这有两个积极的作用，一是能帮助训练“跳出”之前可能卡住它的局部最小值，二是能让训练在"平坦"的最小值结束，这通常会带来更好的泛化性能。

**References:**  
[深度机器学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260) 

2.