---
title: 梯度消失与梯度爆炸
date: 2018-03-20 18:29:12
tags: [Machine Learning]
categories: [Machine Learning]
---

## 1. 梯度不稳定问题
什么是梯度不稳定问题?  
深度神经网络中的梯度不稳定性，前面层中的梯度或会消失，或会爆炸。
**原因**: 前面层上的梯度是来自于后面层上梯度的乘积(后面的公式)。当存在过多的层次时，就出现了内在本质上的不稳定场景，如梯度消失和梯度爆炸。  
目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。  
**整个深度网络可以视为是一个复合的非线性多元函数**：
![F(x)=f_n(...f_3(f_2(f_1(x)∗θ_1+b)∗θ_2+b)...)](http://latex.codecogs.com/gif.latex?F(x)=f_n(...f_3(f_2(f_1(x)%E2%88%97%CE%B8_1+b)%E2%88%97%CE%B8_2+b)...))

## 2. 梯度消失 & 梯度爆炸
**梯度消失(vanishing gradient problem)**与**梯度爆炸(exploding gradient problem)**其实是一种情况。其中，**梯度消失**经常出现，一是在**深层网络**中，二是采用了**不合适的损失函数**(比如sigmoid)。**梯度爆炸**一般出现在**深层网络**和**权值初始化值太大**的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。

### 2.1 深层网络角度
假设每一层网络激活后的输出为f_i(x),其中i为第i层, x代表第i层的输入，也就是第i−1层的输出，f是激活函数，那么，得出f_{i+1}=f(f_{i}∗w_{i+1}+b_{i+1})，简单记为f_{i+1}=f(f_{i}∗w_{i+1})。  
BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新为w←w+Δw，给定学习率α，得出![\Delta{w}=−\alpha\frac{\partial{Loss}}{\partial{w}}]()。如果要更新第二隐藏层的权值信息，根据链式求导法则，更新梯度信息： 
![\Delta w_1=\frac{\partial{Loss}}{\partial{w_2}}=\frac{\partial{Loss}}{\partial{f_4}}\frac{\partial{f_4}}{\partial{f_3}}\frac{\partial{f_3}}{\partial{f_2}}\frac{\partial{f_2}}{\partial{w_2}}](http://latex.codecogs.com/gif.latex?\Delta%20w_1=\frac{\partial{Loss}}{\partial{w_2}}=\frac{\partial{Loss}}{\partial{f_4}}\frac{\partial{f_4}}{\partial{f_3}}\frac{\partial{f_3}}{\partial{f_2}}\frac{\partial{f_2}}{\partial{w_2}})，很容易看出来\frac{∂f_2}{∂w_2}=f_1，即第二隐藏层的输入。 
所以说，∂f4∂f3就是对激活函数进行求导，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失。如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题


对于Sigmoid激活函数，前面的层(离输出层远的层)比后面的层(离输出层近的层)梯度变化更小，故变化更慢，从而引起了梯度消失问题。



梯度爆炸
当权值过大，前面层比后面层梯度变化更快，会引起梯度爆炸问题。

当采用Sigmoid激活函数时，梯度消失和梯度爆炸哪个更易发生？
量化分析梯度爆炸出现时a的取值范围：因为Sigmoid导数最大为1/4，故只有当abs(w)>4时才可能出现
![$ abs(w_j * \sigma_{'}(z_j))>1 $](http://latex.codecogs.com/gif.latex?abs(w_j%20*%20\sigma_{%27}(z_j))%3E1)
由此计算出a的数值变化范围很小，仅仅在此窄范围内会出现梯度爆炸问题。而最普遍发生的是梯度消失问题。



## 5. 如何解决梯度消失和梯度爆炸？
+ 使用ReLU,maxout等替代sigmoid
sigmoid函数的梯度随着x的增大或减小而消失，但ReLU不会


## References
1. [机器学习总结（九）：梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）问题](http://blog.csdn.net/cppjava_/article/details/68941436)
2. [详解梯度爆炸和梯度消失](https://www.cnblogs.com/DLlearning/p/8177273.html)
3. [详解机器学习中的梯度消失、爆炸原因及其解决方法](http://blog.csdn.net/qq_25737169/article/details/78847691)
