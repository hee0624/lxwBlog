---
title: A byte of Feature Engineering
date: 2018-05-28 08:40:49
mathjax: true
tags: [Machine Learning, Feature Engineering]
categories: [Machine Learning, Feature Engineering]
---
> 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已

## 1.数据预处理
无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有**标准化**和**区间缩放法**。**标准化的前提是特征值服从正态分布**，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。  
**标准化 vs 归一化的区别**(程序验证和示例请参见[github代码](https://github.com/lxw0109/ML-Experiments/blob/master/preprocessing_method/src/feature_engineering_sklearn.py)):  
+ 标准化是依照特征矩阵的**列**处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下
+ 归一化是依照特征矩阵的**行**处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是都转化为"单位向量", 例如使用l2范数进行归一化.

**总结如下:**  

| 类	| 功能 | 说明 |
| -- | ---- | ---- |
| StandardScaler | 无量纲化 | 标准化，**基于特征矩阵的列**，将特征值转换至服从标准正态分布 |
| MinMaxScaler | 无量纲化 | 区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上 |
| Normalizer | 归一化 | **基于特征矩阵的行**，将样本向量转换为“单位向量” |
| Binarizer | 二值化 | 基于给定阈值，将定量特征按阈值划分 |
| OneHotEncoder | 哑编码 | 将定性数据编码为定量数据 |
| Imputer | 缺失值计算 | 计算缺失值，缺失值可填充为均值等 |
| PolynomialFeatures | 多项式数据转换 | 多项式数据转换 |
| FunctionTransformer | 自定义单元数据转换 | 使用单变元的函数来转换数据 |

## 2.特征选择
通常从两个方面考虑选择特征:
+ 特征是否发散  
 如果一个特征不发散，如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对于样本的区分并没有什么用
+ 特征与目标的相关性  
 这点比较显然，与目标相关性高的特征，应当优选选择

根据特征选择的形式又可以将特征选择方法分为3种:  
+ Filter: 过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征
+ Wrapper: 包装法，根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征
+ Embedded: 嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

### 2.1 Filter
+ 方差选择法
+ 相关系数法
+ 卡方检验  
 经典的卡方检验是检验**定性自变量**对**定性因变量**的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：
 $$
\chi^2=\sum\frac{(A-E)^2}{E}
 $$
 这个统计量的含义简而言之就是自变量对因变量的相关性。
+ 互信息法(衡量两个随机变量的相关性)

### 2.2 Wrapper
+ 递归特征消除法
 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练

### 2.3 Embedded
+ 基于惩罚项的特征选择法  
 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了**降维**  
 L1惩罚项降维的原理在于**保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要**
+ 基于树模型的特征选择法
 树模型中GBDT也可用来作为基模型进行特征选择

**总结如下:**  

| 类 | 所属方式 | 说明 |
| -- | ---- | ---- |
| VarianceThreshold | Filter | 方差选择法 |
| SelectKBest | Filter | 可选关联系数、卡方校验、最大信息系数作为得分计算的方法 |
| RFE | Wrapper | 递归地训练基模型，将权值系数较小的特征从特征集合中消除 |
| SelectFromModel | Embedded | 训练基模型，选择权值系数较高的特征 |

## 3.降维
当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。**常见的降维方法**除了以上提到的**基于L1惩罚项的模型**以外，另外还有**主成分分析法(PCA)**和**线性判别分析(LDA)**，线性判别分析本身也是一个分类模型。  
PCA和LDA有很多的相似点，其**本质是要将原始的样本映射到维度更低的样本空间中**，但是PCA和LDA的映射目标不一样：**PCA是为了让映射后的样本具有最大的发散性**；而**LDA是为了让映射后的样本有最好的分类性能**。所以说**PCA**是一种**无监督**的降维方法，而**LDA**是一种**有监督**的降维方法。

## References
1. [使用sklearn做单机特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)
